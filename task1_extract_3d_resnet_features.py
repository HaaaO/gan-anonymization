# -*- coding: utf-8 -*-
"""task1_extract 3d_resnet features.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15ekCVgw6WRUhKl01DbetY58h2zYGg636
"""

!pip install av
!pip install pytorchvideo
!pip install fvcore

import json
import os

!nvidia-smi

from google.colab import drive
drive.mount('/content/drive')

path_to_test = "/content/drive/MyDrive/HMDB/PA-HMDB51/"
path_to_train = "/content/drive/MyDrive/HMDB/videos/"

action_list = ['ride_bike', 'kick', 'smoke', 'sword', 'golf', 'climb', 'walk', 'sword_exercise', 'hit', 'swing_baseball', 'brush_hair', 'ride_horse', 'dive', 'shoot_bow', 'sit', 'kiss', 'jump', 'hug', 'pushup', 'turn', 'throw', 'somersault', 'push', 'eat', 'shake_hands', 'pullup', 'climb_stairs', 'cartwheel', 'fencing', 'pour', 'clap', 'draw_sword', 'catch', 'kick_ball', 'stand', 'punch', 'wave', 'chew', 'run', 'shoot_ball', 'shoot_gun', 'situp', 'handstand', 'dribble', 'fall_floor', 'flic_flac', 'drink', 'pick', 'smile', 'laugh', 'talk']
train_list = []
test_list = []

all = []
for action in action_list:

  test_set = set()
  test_path = path_to_test + action + ".json"

  f = open(test_path)
  data = json.load(f)
  for video in data.keys():
    test_list.append([action + "/"+ video, action])
    test_set.add(video)

  train_path = path_to_train + action 

  for subdir, dirs, files in os.walk(train_path):
      for file in files:
        all.append(file)
        if file not in test_set:
          train_list.append([action + "/"+ file, action])

print(len(train_list))
print(len(test_list))
print(len(all))
print(train_list)
print(test_list)

import torch.nn as nn
class Identity(nn.Module):
    def __init__(self):
        super(Identity, self).__init__()
        
    def forward(self, x):
        return x

import torchvision
from torchvision.io.video import read_video
from torchvision.models.video import r2plus1d_18, R2Plus1D_18_Weights

# vid, _, _ = read_video("20060723sfjffjewcy_run_f_nm_np1_ba_med_3.avi", output_format="TCHW")
# vid = vid[:32]  # optionally shorten duration
print()
# Step 1: Initialize model with the best available weights
weights = R2Plus1D_18_Weights.DEFAULT
model = r2plus1d_18(weights = R2Plus1D_18_Weights.KINETICS400_V1)
# model = torch.hub.load("pytorch/vision", "r2plus1d_18", weights="R2Plus1D_18_Weights")
model.fc = Identity()

preprocess = weights.transforms()

"""#### Setup

Set the model to eval mode and move to desired device.
"""

# Set to GPU or CPU
device = "cuda"
model = model.eval()
model = model.to(device)

len(train_list)



"""Load the video and transform it to the input format required by the model."""

import os.path
import numpy as np
import torchvision
import av
import torch
from numba import cuda 

# device = cuda.get_current_device()
# device.reset()
train_feature_list = []

action_set = set()
with open("train_list.txt", "w") as output:
  for video_list in train_list:
    print(video_list[0])
    video_path = "/content/drive/MyDrive/HMDB/videos/" + video_list[0]


    if os.path.isfile(video_path): 
      with torch.no_grad():
        video, _, _ = read_video(video_path, output_format="TCHW")
        # print(video)
        output.write(video_list[0] + "" + video_list[1] + "\n")

        inputs = preprocess(video)
  
        inputs = inputs.to(device)

        feat = model(inputs[None, ...]).cpu().detach().numpy()

        train_feature_list.append(feat)
   
 
    

np_train_feature_list = np.concatenate( train_feature_list, axis=0 )
print(np_train_feature_list.shape)
torch.save(np_train_feature_list, "train_feature.pkl")

test_feature_list = []
action_set = set()
with open("test_list.txt", "w") as output:
  for video_list in test_list:
    print(video_list[0])
    video_path = "/content/drive/MyDrive/HMDB/videos/" + video_list[0]


    if os.path.isfile(video_path): 
      with torch.no_grad():
        video, _, _ = read_video(video_path, output_format="TCHW")
        # print(video)
        output.write(video_list[0] + "," + video_list[1] + "\n")

        inputs = preprocess(video)
  
        inputs = inputs.to(device)

        feat = model(inputs[None, ...]).cpu().detach().numpy()

        test_feature_list.append(feat)
   
 
    

np_test_feature_list = np.concatenate( test_feature_list, axis=0 )
print(np_test_feature_list.shape)
torch.save(np_test_feature_list, "test_feature.pkl")

"""#### Get Predictions"""

# feat = model(inputs[None, ...]).cpu().detach().numpy()
# # feat = model(inputs)
# print(feat.shape)
# print(feat)

# l2 norm to the feature
# Any other processing? Normalizaiton?

# # Pass the input clip through the model
# preds = model(inputs)

# # Get the predicted classes
# post_act = torch.nn.Softmax(dim=1)
# preds = post_act(preds)
# pred_classes = preds.topk(k=5).indices[0]

# # Map the predicted classes to the label names
# pred_class_names = [kinetics_id_to_classname[int(i)] for i in pred_classes]
# print("Top 5 predicted labels: %s" % ", ".join(pred_class_names))

"""### Model Description
The model architecture is based on [1] with pretrained weights using the 8x8 setting
on the Kinetics dataset. 
| arch | depth | frame length x sample rate | top 1 | top 5 | Flops (G) | Params (M) |
| --------------- | ----------- | ----------- | ----------- | ----------- | ----------- |  ----------- | ----------- |
| Slow     | R50   | 8x8                        | 74.58 | 91.63 | 54.52     | 32.45     |


### References
[1] Christoph Feichtenhofer et al, "SlowFast Networks for Video Recognition"
https://arxiv.org/pdf/1812.03982.pdf
"""

